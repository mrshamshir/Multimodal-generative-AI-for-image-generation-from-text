# Multimodal generative AI for image generation from text

This project introduces an innovative multimodal model designed to generate images from text
prompts. Utilizing two state-of-the-art open-source pretrained models, we seamlessly connected them using Python
and PyTorch. CLIP (Contrastive Language-Image Pre-Training) is employed for encoding text prompts and images,
while image generation is facilitated by a VQGAN transformer model (Taming Transformers for High-Resolution
Image Synthesis). Through the definition of a loss function and the creation of a new latent space, we achieved the
capability to generate images for diverse text prompts in a trainable process.

Note: this repository and its README will be completed over time.
